{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e1746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 13769.88it/s]\n",
      "/Users/thomaslim/miniconda3/envs/kestrel/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m      7\u001b[39m input_text = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33mAlthough it varies with age and geographical distribution, the global burden of infection with Streptococcus pneumoniae (pneumococcus) remains considerable. \u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33mThe elderly, and younger adults with comorbid conditions, are at particularly high risk of pneumococcal infection, and this risk will increase as the population ages. \u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33mConclusion: While there has been enormous progress in the development of pneumococcal vaccines during the past century, attempts to develop a vaccine that will retain its efficacy for most pneumococcal serotypes are ongoing.\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# GliNER mode\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m model = \u001b[43mGLiNER\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murchade/gliner_mediumv2.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m model.eval()\n\u001b[32m     28\u001b[39m labels = [\u001b[33m\"\u001b[39m\u001b[33mvaccine\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpathogen\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/huggingface_hub/hub_mixin.py:568\u001b[39m, in \u001b[36mModelHubMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[39m\n\u001b[32m    565\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._hub_mixin_inject_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[32m    566\u001b[39m         model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m] = config\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m instance = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# Implicitly set the config as instance attribute if not already set by the class\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# This way `config` will be available when calling `save_pretrained` or `push_to_hub`.\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mgetattr\u001b[39m(instance, \u001b[33m\"\u001b[39m\u001b[33m_hub_mixin_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, {})):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/gliner/model.py:862\u001b[39m, in \u001b[36mGLiNER._from_pretrained\u001b[39m\u001b[34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, load_tokenizer, resize_token_embeddings, load_onnx_model, onnx_model_file, compile_torch_model, session_options, _attn_implementation, max_length, max_width, post_fusion_schema, **model_kwargs)\u001b[39m\n\u001b[32m    859\u001b[39m add_tokens = [\u001b[33m\"\u001b[39m\u001b[33m[FLERT]\u001b[39m\u001b[33m\"\u001b[39m, config.ent_token, config.sep_token]\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m load_onnx_model:\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     gliner = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_from_pretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m     \u001b[38;5;66;03m# to be able to load GLiNER models from previous version\u001b[39;00m\n\u001b[32m    864\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    865\u001b[39m         config.class_token_index == -\u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m config.vocab_size == -\u001b[32m1\u001b[39m\n\u001b[32m    866\u001b[39m     ) \u001b[38;5;129;01mand\u001b[39;00m resize_token_embeddings \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.labels_encoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/gliner/model.py:77\u001b[39m, in \u001b[36mGLiNER.__init__\u001b[39m\u001b[34m(self, config, model, tokenizer, words_splitter, data_processor, encoder_from_pretrained, cache_dir)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m         \u001b[38;5;28mself\u001b[39m.model = \u001b[43mSpanModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_from_pretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     79\u001b[39m         \u001b[38;5;28mself\u001b[39m.model = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/gliner/modeling/base.py:395\u001b[39m, in \u001b[36mSpanModel.__init__\u001b[39m\u001b[34m(self, config, encoder_from_pretrained, cache_dir)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, encoder_from_pretrained, cache_dir: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path]] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSpanModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_from_pretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m     \u001b[38;5;28mself\u001b[39m.span_rep_layer = SpanRepLayer(span_mode = config.span_mode, \n\u001b[32m    397\u001b[39m                                        hidden_size = config.hidden_size, \n\u001b[32m    398\u001b[39m                                        max_width = config.max_width,\n\u001b[32m    399\u001b[39m                                        dropout = config.dropout)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28mself\u001b[39m.prompt_rep_layer = create_projection_layer(config.hidden_size, config.dropout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/gliner/modeling/base.py:92\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, config, from_pretrained, cache_dir)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.labels_encoder:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28mself\u001b[39m.token_rep_layer = \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mself\u001b[39m.token_rep_layer = BiEncoder(config, from_pretrained, cache_dir = cache_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/gliner/modeling/encoder.py:131\u001b[39m, in \u001b[36mEncoder.__init__\u001b[39m\u001b[34m(self, config, from_pretrained, cache_dir)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, from_pretrained: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, cache_dir: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path]]= \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    129\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28mself\u001b[39m.bert_layer = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#transformer_model\u001b[39;49;00m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     bert_hidden_size = \u001b[38;5;28mself\u001b[39m.bert_layer.model.config.hidden_size\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.hidden_size != bert_hidden_size:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/gliner/modeling/encoder.py:91\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name, config, from_pretrained, labels_encoder, cache_dir)\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[38;5;28mself\u001b[39m.model = ModelClass.from_config(encoder_config, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28mself\u001b[39m.model = \u001b[43mModelClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m adapter_config_file = Path(model_name) / \u001b[33m\"\u001b[39m\u001b[33madapter_config.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adapter_config_file.exists():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:723\u001b[39m, in \u001b[36mDebertaV2Model.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m    721\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m     \u001b[38;5;28mself\u001b[39m.embeddings = \u001b[43mDebertaV2Embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    724\u001b[39m     \u001b[38;5;28mself\u001b[39m.encoder = DebertaV2Encoder(config)\n\u001b[32m    725\u001b[39m     \u001b[38;5;28mself\u001b[39m.z_steps = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:499\u001b[39m, in \u001b[36mDebertaV2Embeddings.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    497\u001b[39m pad_token_id = \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mpad_token_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m    498\u001b[39m \u001b[38;5;28mself\u001b[39m.embedding_size = \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33membedding_size\u001b[39m\u001b[33m\"\u001b[39m, config.hidden_size)\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m \u001b[38;5;28mself\u001b[39m.word_embeddings = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28mself\u001b[39m.position_biased_input = \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mposition_biased_input\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.position_biased_input:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/torch/nn/modules/sparse.py:172\u001b[39m, in \u001b[36mEmbedding.__init__\u001b[39m\u001b[34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m    169\u001b[39m         torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n\u001b[32m    170\u001b[39m         requires_grad=\u001b[38;5;129;01mnot\u001b[39;00m _freeze,\n\u001b[32m    171\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_weight.shape) == [\n\u001b[32m    175\u001b[39m         num_embeddings,\n\u001b[32m    176\u001b[39m         embedding_dim,\n\u001b[32m    177\u001b[39m     ], \u001b[33m\"\u001b[39m\u001b[33mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/torch/nn/modules/sparse.py:183\u001b[39m, in \u001b[36mEmbedding.reset_parameters\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[43minit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28mself\u001b[39m._fill_padding_idx_with_zero()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/torch/nn/init.py:264\u001b[39m, in \u001b[36mnormal_\u001b[39m\u001b[34m(tensor, mean, std, generator)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.overrides.has_torch_function_variadic(tensor):\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.overrides.handle_torch_function(\n\u001b[32m    262\u001b[39m         normal_, (tensor,), tensor=tensor, mean=mean, std=std, generator=generator\n\u001b[32m    263\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kestrel/lib/python3.11/site-packages/torch/nn/init.py:82\u001b[39m, in \u001b[36m_no_grad_normal_\u001b[39m\u001b[34m(tensor, mean, std, generator)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_normal_\u001b[39m(\n\u001b[32m     76\u001b[39m     tensor: Tensor,\n\u001b[32m     77\u001b[39m     mean: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m     78\u001b[39m     std: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m     79\u001b[39m     generator: _Optional[torch.Generator] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     80\u001b[39m ) -> Tensor:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "import json\n",
    "from transformers import pipeline\n",
    "from gliner_to_docred import convert_gliner_to_docred\n",
    "\n",
    "# Input text\n",
    "input_text = \"\"\"\n",
    "Although it varies with age and geographical distribution, the global burden of infection with Streptococcus pneumoniae (pneumococcus) remains considerable. \n",
    "The elderly, and younger adults with comorbid conditions, are at particularly high risk of pneumococcal infection, and this risk will increase as the population ages. \n",
    "Vaccination should be the backbone of our current strategies to deal with this infection.\n",
    "Main body: This manuscript reviews the history of the development of pneumococcal vaccines, and the impact of different vaccines and vaccination strategies over the past 111 years. \n",
    "It documents the early years of vaccine development in the gold mines of South Africa, when vaccination with killed pneumococci was shown to be effective, even before the recognition that different pneumococci were antigenically distinct. \n",
    "The development of type-specific vaccines, still with whole killed pneumococci, showed a high degree of efficacy. \n",
    "The identification of the importance of the pneumococcal capsule heralded the era of vaccination with capsular polysaccharides, although with the advent of penicillin, interest in pneumococcal vaccine development waned. \n",
    "The efforts of Austrian and his colleagues, who documented that despite penicillin therapy, patients still died from pneumococcal infection in the first 96 h, ultimately led to the licensing first of a 14-valent pneumococcal polysaccharide in 1977 followed by the 23-valent pneumococcal polysaccharide in 1983. \n",
    "The principal problem with these, as with other polysaccharide vaccines, was that that they failed to immunize infants and toddlers, who were at highest risk for pneumococcal disease. \n",
    "This was overcome by chemical linking or conjugation of the polysaccharide molecules to an immunogenic carrier protein. \n",
    "Thus began the era of pneumococcal conjugate vaccine (PCV), starting with PCV7, progressing to PCV10 and PCV13, and, most recently, PCV15 and PCV20. \n",
    "However, these vaccines remain serotype specific, posing the challenge of new serotypes replacing vaccine types. \n",
    "Current research addresses serotype-independent vaccines which, so far, has been a challenging and elusive endeavor.\n",
    "Conclusion: While there has been enormous progress in the development of pneumococcal vaccines during the past century, attempts to develop a vaccine that will retain its efficacy for most pneumococcal serotypes are ongoing.\n",
    "\"\"\"\n",
    "\n",
    "# GliNER mode\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_mediumv2.1\")\n",
    "model.eval()\n",
    "\n",
    "labels = [\"vaccine\", \"pathogen\"]\n",
    "\n",
    "entities = model.predict_entities(input_text, labels, threshold=0.4)\n",
    "\n",
    "# for entity in entities:\n",
    "#     print(entity[\"text\"], \"=>\", entity[\"label\"])\n",
    "\n",
    "# Convert to DocRED format\n",
    "input_preprocessed_to_docred = convert_gliner_to_docred(input_text, entities)\n",
    "\n",
    "\n",
    "# Vaccine-Pathogen Relation Model\n",
    "vaccine_pathogen_model_name = \"thomaslim6793/vaccine-pathogen-model\"\n",
    "vaccine_pathogen_pipe = pipeline(\"vaccine-pathogen-relation\", model=vaccine_pathogen_model_name, trust_remote_code=True)\n",
    "\n",
    "# Run prediction\n",
    "vaccine_pathogen_results = vaccine_pathogen_pipe(input_preprocessed_to_docred)\n",
    "\n",
    "\n",
    "# filter by where \"predicted_relation\" is \"vaccine_targets\"\n",
    "for doc_result in vaccine_pathogen_results:\n",
    "    for pred in doc_result['predictions']:\n",
    "        if pred['predicted_relation'] == 'vaccine_targets':\n",
    "            print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795d3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([entity for entity in entities if entity[\"label\"] == \"vaccine\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kestrel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
